{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1  A2  A3  A4  A5   A6  A7  A8  A9  A10  A11  A12  Class\n",
       "0   1   1   2  40  10  101   1   5   2    1    6    2      5\n",
       "1   1   1   2  43  10    7  10   1  44   10    2   44      5\n",
       "2   1   1  43  40  10   35  11   4  45   11    4   45     20\n",
       "3   1   1  43  41  10   13  10   7  42   10    7   42      5\n",
       "4   1   1  43  43  10   44  12   6  49   12    6   49     20"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "urlTraining = 'https://raw.githubusercontent.com/gbrian/machinelearning_exercises/master/ml/supervised/exercise_1/test.tsv'\n",
    "urlTest = 'https://raw.githubusercontent.com/gbrian/machinelearning_exercises/master/ml/supervised/exercise_1/test.tsv'\n",
    "orgtraining = pandas.read_csv(urlTraining, delimiter='\\t')\n",
    "orgtesting = pandas.read_csv(urlTest, delimiter='\\t')\n",
    "orgtraining.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "https://machinelearningmastery.com/feature-selection-machine-learning-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing = orgtesting.drop('A1', axis=1)\n",
    "training = orgtraining.drop('A1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   column         score\n",
      "0      A1  6.922209e+07\n",
      "1      A2  1.176584e+07\n",
      "2      A3  9.947493e+06\n",
      "3      A4  3.012508e+05\n",
      "4      A6  1.659314e+05\n",
      "5      A9  7.330293e+04\n",
      "6      A5  6.170595e+04\n",
      "7      A7  1.552336e+04\n",
      "8     A10  1.549642e+04\n",
      "9      A8  2.239109e+02\n",
      "10    A11  1.973365e+02\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "classIndex = len(training.columns)-1\n",
    "lastColIndex = classIndex-1\n",
    "array = training.values\n",
    "X = array[:,0:-2]\n",
    "Y = array[:,-1]\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "# summarize scores\n",
    "columns = list(training.columns)\n",
    "numpy.set_printoptions(precision=3)\n",
    "scoredFeatures = [{\"score\":x,\"column\":columns[i]} for i,x in enumerate(fit.scores_)]\n",
    "sortedScoredFeatures = sorted(scoredFeatures, key=lambda x: x['score'], reverse=True) \n",
    "print(pandas.DataFrame.from_dict(sortedScoredFeatures))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "### auto_ml\n",
    "http://auto-ml.readthedocs.io/en/latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at https://github.com/ClimbsRocks/auto_ml\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'presort': False, 'learning_rate': 0.1, 'warm_start': True}\n",
      "Running basic data cleaning\n",
      "Performing feature scaling\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'presort': False, 'learning_rate': 0.1, 'warm_start': True}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model GradientBoostingClassifier to predict Class\n",
      "Started at:\n",
      "2017-11-04 09:51:58\n",
      "[1] random_holdout_set_from_training_data's score is: 0.601\n",
      "[2] random_holdout_set_from_training_data's score is: 0.616\n",
      "[3] random_holdout_set_from_training_data's score is: 0.618\n",
      "[4] random_holdout_set_from_training_data's score is: 0.62\n",
      "[5] random_holdout_set_from_training_data's score is: 0.619\n",
      "[6] random_holdout_set_from_training_data's score is: 0.625\n",
      "[7] random_holdout_set_from_training_data's score is: 0.627\n",
      "[8] random_holdout_set_from_training_data's score is: 0.625\n",
      "[9] random_holdout_set_from_training_data's score is: 0.628\n",
      "[10] random_holdout_set_from_training_data's score is: 0.632\n",
      "[11] random_holdout_set_from_training_data's score is: 0.635\n",
      "[12] random_holdout_set_from_training_data's score is: 0.635\n",
      "[13] random_holdout_set_from_training_data's score is: 0.637\n",
      "[14] random_holdout_set_from_training_data's score is: 0.635\n",
      "[15] random_holdout_set_from_training_data's score is: 0.639\n",
      "[16] random_holdout_set_from_training_data's score is: 0.638\n",
      "[17] random_holdout_set_from_training_data's score is: 0.639\n",
      "[18] random_holdout_set_from_training_data's score is: 0.643\n",
      "[19] random_holdout_set_from_training_data's score is: 0.643\n",
      "[20] random_holdout_set_from_training_data's score is: 0.645\n",
      "[21] random_holdout_set_from_training_data's score is: 0.646\n",
      "[22] random_holdout_set_from_training_data's score is: 0.648\n",
      "[23] random_holdout_set_from_training_data's score is: 0.647\n",
      "[24] random_holdout_set_from_training_data's score is: 0.649\n",
      "[25] random_holdout_set_from_training_data's score is: 0.648\n",
      "[26] random_holdout_set_from_training_data's score is: 0.649\n",
      "[27] random_holdout_set_from_training_data's score is: 0.649\n",
      "[28] random_holdout_set_from_training_data's score is: 0.65\n",
      "[29] random_holdout_set_from_training_data's score is: 0.65\n",
      "[30] random_holdout_set_from_training_data's score is: 0.651\n",
      "[31] random_holdout_set_from_training_data's score is: 0.652\n",
      "[32] random_holdout_set_from_training_data's score is: 0.654\n",
      "[33] random_holdout_set_from_training_data's score is: 0.654\n",
      "[34] random_holdout_set_from_training_data's score is: 0.652\n",
      "[35] random_holdout_set_from_training_data's score is: 0.656\n",
      "[36] random_holdout_set_from_training_data's score is: 0.658\n",
      "[37] random_holdout_set_from_training_data's score is: 0.66\n",
      "[38] random_holdout_set_from_training_data's score is: 0.659\n",
      "[39] random_holdout_set_from_training_data's score is: 0.661\n",
      "[40] random_holdout_set_from_training_data's score is: 0.662\n",
      "[41] random_holdout_set_from_training_data's score is: 0.666\n",
      "[42] random_holdout_set_from_training_data's score is: 0.666\n",
      "[43] random_holdout_set_from_training_data's score is: 0.666\n",
      "[44] random_holdout_set_from_training_data's score is: 0.666\n",
      "[45] random_holdout_set_from_training_data's score is: 0.668\n",
      "[46] random_holdout_set_from_training_data's score is: 0.668\n",
      "[47] random_holdout_set_from_training_data's score is: 0.669\n",
      "[48] random_holdout_set_from_training_data's score is: 0.669\n",
      "[49] random_holdout_set_from_training_data's score is: 0.67\n",
      "[50] random_holdout_set_from_training_data's score is: 0.67\n",
      "[52] random_holdout_set_from_training_data's score is: 0.671\n",
      "[54] random_holdout_set_from_training_data's score is: 0.673\n",
      "[56] random_holdout_set_from_training_data's score is: 0.675\n",
      "[58] random_holdout_set_from_training_data's score is: 0.674\n",
      "[60] random_holdout_set_from_training_data's score is: 0.674\n",
      "[62] random_holdout_set_from_training_data's score is: 0.677\n",
      "[64] random_holdout_set_from_training_data's score is: 0.679\n",
      "[66] random_holdout_set_from_training_data's score is: 0.678\n",
      "[68] random_holdout_set_from_training_data's score is: 0.678\n",
      "[70] random_holdout_set_from_training_data's score is: 0.678\n",
      "[72] random_holdout_set_from_training_data's score is: 0.679\n",
      "[74] random_holdout_set_from_training_data's score is: 0.68\n",
      "[76] random_holdout_set_from_training_data's score is: 0.679\n",
      "[78] random_holdout_set_from_training_data's score is: 0.679\n",
      "[80] random_holdout_set_from_training_data's score is: 0.68\n",
      "[82] random_holdout_set_from_training_data's score is: 0.681\n",
      "[84] random_holdout_set_from_training_data's score is: 0.682\n",
      "[86] random_holdout_set_from_training_data's score is: 0.683\n",
      "[88] random_holdout_set_from_training_data's score is: 0.682\n",
      "[90] random_holdout_set_from_training_data's score is: 0.683\n",
      "[92] random_holdout_set_from_training_data's score is: 0.683\n",
      "[94] random_holdout_set_from_training_data's score is: 0.684\n",
      "[96] random_holdout_set_from_training_data's score is: 0.684\n",
      "[98] random_holdout_set_from_training_data's score is: 0.684\n",
      "[100] random_holdout_set_from_training_data's score is: 0.684\n",
      "[103] random_holdout_set_from_training_data's score is: 0.685\n",
      "[106] random_holdout_set_from_training_data's score is: 0.684\n",
      "[109] random_holdout_set_from_training_data's score is: 0.684\n",
      "[112] random_holdout_set_from_training_data's score is: 0.685\n",
      "[115] random_holdout_set_from_training_data's score is: 0.684\n",
      "[118] random_holdout_set_from_training_data's score is: 0.684\n",
      "[121] random_holdout_set_from_training_data's score is: 0.685\n",
      "[124] random_holdout_set_from_training_data's score is: 0.686\n",
      "[127] random_holdout_set_from_training_data's score is: 0.685\n",
      "[130] random_holdout_set_from_training_data's score is: 0.685\n",
      "[133] random_holdout_set_from_training_data's score is: 0.686\n",
      "[136] random_holdout_set_from_training_data's score is: 0.687\n",
      "[139] random_holdout_set_from_training_data's score is: 0.687\n",
      "[142] random_holdout_set_from_training_data's score is: 0.689\n",
      "[145] random_holdout_set_from_training_data's score is: 0.689\n",
      "[148] random_holdout_set_from_training_data's score is: 0.691\n",
      "[151] random_holdout_set_from_training_data's score is: 0.69\n",
      "[154] random_holdout_set_from_training_data's score is: 0.69\n",
      "[157] random_holdout_set_from_training_data's score is: 0.691\n",
      "[160] random_holdout_set_from_training_data's score is: 0.69\n",
      "[163] random_holdout_set_from_training_data's score is: 0.691\n",
      "[166] random_holdout_set_from_training_data's score is: 0.692\n",
      "[169] random_holdout_set_from_training_data's score is: 0.691\n",
      "[172] random_holdout_set_from_training_data's score is: 0.691\n",
      "[175] random_holdout_set_from_training_data's score is: 0.692\n",
      "[178] random_holdout_set_from_training_data's score is: 0.692\n",
      "[181] random_holdout_set_from_training_data's score is: 0.693\n",
      "[184] random_holdout_set_from_training_data's score is: 0.693\n",
      "[187] random_holdout_set_from_training_data's score is: 0.693\n",
      "[190] random_holdout_set_from_training_data's score is: 0.694\n",
      "[193] random_holdout_set_from_training_data's score is: 0.694\n",
      "[196] random_holdout_set_from_training_data's score is: 0.694\n",
      "[199] random_holdout_set_from_training_data's score is: 0.694\n",
      "[202] random_holdout_set_from_training_data's score is: 0.696\n",
      "[205] random_holdout_set_from_training_data's score is: 0.696\n",
      "[208] random_holdout_set_from_training_data's score is: 0.696\n",
      "[211] random_holdout_set_from_training_data's score is: 0.694\n",
      "[214] random_holdout_set_from_training_data's score is: 0.695\n",
      "[217] random_holdout_set_from_training_data's score is: 0.696\n",
      "[220] random_holdout_set_from_training_data's score is: 0.697\n",
      "[223] random_holdout_set_from_training_data's score is: 0.697\n",
      "[226] random_holdout_set_from_training_data's score is: 0.698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[229] random_holdout_set_from_training_data's score is: 0.698\n",
      "[232] random_holdout_set_from_training_data's score is: 0.698\n",
      "[235] random_holdout_set_from_training_data's score is: 0.699\n",
      "[238] random_holdout_set_from_training_data's score is: 0.698\n",
      "[241] random_holdout_set_from_training_data's score is: 0.699\n",
      "[244] random_holdout_set_from_training_data's score is: 0.699\n",
      "[247] random_holdout_set_from_training_data's score is: 0.699\n",
      "[250] random_holdout_set_from_training_data's score is: 0.699\n",
      "[255] random_holdout_set_from_training_data's score is: 0.7\n",
      "[260] random_holdout_set_from_training_data's score is: 0.701\n",
      "[265] random_holdout_set_from_training_data's score is: 0.701\n",
      "[270] random_holdout_set_from_training_data's score is: 0.702\n",
      "[275] random_holdout_set_from_training_data's score is: 0.704\n",
      "[280] random_holdout_set_from_training_data's score is: 0.703\n",
      "[285] random_holdout_set_from_training_data's score is: 0.702\n",
      "[290] random_holdout_set_from_training_data's score is: 0.702\n",
      "[295] random_holdout_set_from_training_data's score is: 0.702\n",
      "[300] random_holdout_set_from_training_data's score is: 0.702\n",
      "[305] random_holdout_set_from_training_data's score is: 0.703\n",
      "[310] random_holdout_set_from_training_data's score is: 0.702\n",
      "[315] random_holdout_set_from_training_data's score is: 0.702\n",
      "[320] random_holdout_set_from_training_data's score is: 0.704\n",
      "[325] random_holdout_set_from_training_data's score is: 0.704\n",
      "[330] random_holdout_set_from_training_data's score is: 0.702\n",
      "[335] random_holdout_set_from_training_data's score is: 0.704\n",
      "[340] random_holdout_set_from_training_data's score is: 0.704\n",
      "[345] random_holdout_set_from_training_data's score is: 0.704\n",
      "[350] random_holdout_set_from_training_data's score is: 0.704\n",
      "[355] random_holdout_set_from_training_data's score is: 0.704\n",
      "[360] random_holdout_set_from_training_data's score is: 0.705\n",
      "[365] random_holdout_set_from_training_data's score is: 0.705\n",
      "[370] random_holdout_set_from_training_data's score is: 0.706\n",
      "[375] random_holdout_set_from_training_data's score is: 0.706\n",
      "[380] random_holdout_set_from_training_data's score is: 0.706\n",
      "[385] random_holdout_set_from_training_data's score is: 0.706\n",
      "[390] random_holdout_set_from_training_data's score is: 0.706\n",
      "[395] random_holdout_set_from_training_data's score is: 0.707\n",
      "[400] random_holdout_set_from_training_data's score is: 0.708\n",
      "[405] random_holdout_set_from_training_data's score is: 0.708\n",
      "[410] random_holdout_set_from_training_data's score is: 0.708\n",
      "[415] random_holdout_set_from_training_data's score is: 0.708\n",
      "[420] random_holdout_set_from_training_data's score is: 0.709\n",
      "[425] random_holdout_set_from_training_data's score is: 0.709\n",
      "[430] random_holdout_set_from_training_data's score is: 0.71\n",
      "[435] random_holdout_set_from_training_data's score is: 0.71\n",
      "[440] random_holdout_set_from_training_data's score is: 0.71\n",
      "[445] random_holdout_set_from_training_data's score is: 0.711\n",
      "[450] random_holdout_set_from_training_data's score is: 0.711\n",
      "[455] random_holdout_set_from_training_data's score is: 0.712\n",
      "[460] random_holdout_set_from_training_data's score is: 0.711\n",
      "[465] random_holdout_set_from_training_data's score is: 0.712\n",
      "[470] random_holdout_set_from_training_data's score is: 0.714\n",
      "[475] random_holdout_set_from_training_data's score is: 0.714\n",
      "[480] random_holdout_set_from_training_data's score is: 0.715\n",
      "[485] random_holdout_set_from_training_data's score is: 0.715\n",
      "[490] random_holdout_set_from_training_data's score is: 0.715\n",
      "[495] random_holdout_set_from_training_data's score is: 0.713\n",
      "[500] random_holdout_set_from_training_data's score is: 0.713\n",
      "[510] random_holdout_set_from_training_data's score is: 0.712\n",
      "[520] random_holdout_set_from_training_data's score is: 0.712\n",
      "[530] random_holdout_set_from_training_data's score is: 0.713\n",
      "[540] random_holdout_set_from_training_data's score is: 0.713\n",
      "[550] random_holdout_set_from_training_data's score is: 0.712\n",
      "[560] random_holdout_set_from_training_data's score is: 0.713\n",
      "[570] random_holdout_set_from_training_data's score is: 0.715\n",
      "[580] random_holdout_set_from_training_data's score is: 0.714\n",
      "[590] random_holdout_set_from_training_data's score is: 0.714\n",
      "[600] random_holdout_set_from_training_data's score is: 0.714\n",
      "[610] random_holdout_set_from_training_data's score is: 0.716\n",
      "[620] random_holdout_set_from_training_data's score is: 0.715\n",
      "[630] random_holdout_set_from_training_data's score is: 0.716\n",
      "[640] random_holdout_set_from_training_data's score is: 0.716\n",
      "[650] random_holdout_set_from_training_data's score is: 0.714\n",
      "[660] random_holdout_set_from_training_data's score is: 0.715\n",
      "[670] random_holdout_set_from_training_data's score is: 0.715\n",
      "[680] random_holdout_set_from_training_data's score is: 0.717\n",
      "[690] random_holdout_set_from_training_data's score is: 0.716\n",
      "[700] random_holdout_set_from_training_data's score is: 0.717\n",
      "[710] random_holdout_set_from_training_data's score is: 0.717\n",
      "[720] random_holdout_set_from_training_data's score is: 0.716\n",
      "[730] random_holdout_set_from_training_data's score is: 0.715\n",
      "[740] random_holdout_set_from_training_data's score is: 0.717\n",
      "[750] random_holdout_set_from_training_data's score is: 0.717\n",
      "[760] random_holdout_set_from_training_data's score is: 0.717\n",
      "[770] random_holdout_set_from_training_data's score is: 0.716\n",
      "[780] random_holdout_set_from_training_data's score is: 0.716\n",
      "[790] random_holdout_set_from_training_data's score is: 0.715\n",
      "[800] random_holdout_set_from_training_data's score is: 0.715\n",
      "[810] random_holdout_set_from_training_data's score is: 0.715\n",
      "[820] random_holdout_set_from_training_data's score is: 0.715\n",
      "[830] random_holdout_set_from_training_data's score is: 0.716\n",
      "[840] random_holdout_set_from_training_data's score is: 0.715\n",
      "[850] random_holdout_set_from_training_data's score is: 0.716\n",
      "[860] random_holdout_set_from_training_data's score is: 0.715\n",
      "[870] random_holdout_set_from_training_data's score is: 0.714\n",
      "[880] random_holdout_set_from_training_data's score is: 0.715\n",
      "[890] random_holdout_set_from_training_data's score is: 0.715\n",
      "[900] random_holdout_set_from_training_data's score is: 0.715\n",
      "[910] random_holdout_set_from_training_data's score is: 0.715\n",
      "[920] random_holdout_set_from_training_data's score is: 0.714\n",
      "[930] random_holdout_set_from_training_data's score is: 0.713\n",
      "[940] random_holdout_set_from_training_data's score is: 0.714\n",
      "The number of estimators that were the best for this training dataset: 740\n",
      "The best score on the holdout set: 0.717321997875\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:27:40\n",
      "\n",
      "\n",
      "Here are the results from our GradientBoostingClassifier\n",
      "predicting Class\n",
      "Calculating feature responses, for advanced analytics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gustavo.brian\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The printed list will only contain at most the top 100 features.\n",
      "The full analytics results will be saved to a filed called: auto_ml_analytics_results_Class.csv\n",
      "\n",
      "+--------+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "|        | Feature Name   |   Importance |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_abs |   FRI_abs |   FRD_MAP |   FRI_MAP |\n",
      "|--------+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------|\n",
      "| 9.0000 | A7             |       0.0081 |   2.0897 |            0.0023 |           -0.0010 |    0.0049 |    0.0031 |    0.0003 |    0.0002 |\n",
      "| 8.0000 | A10            |       0.0100 |   2.0916 |           -0.0023 |           -0.0001 |    0.0049 |    0.0040 |    0.0003 |    0.0003 |\n",
      "| 7.0000 | A11            |       0.0344 |   1.0142 |            0.0020 |           -0.0025 |    0.0064 |    0.0079 |    0.0011 |    0.0013 |\n",
      "| 6.0000 | A4             |       0.0412 |   0.1847 |            0.0037 |            0.0087 |    0.0096 |    0.0267 |    0.0017 |    0.0052 |\n",
      "| 5.0000 | A8             |       0.0435 |   1.0149 |           -0.0032 |            0.0012 |    0.0087 |    0.0059 |    0.0017 |    0.0009 |\n",
      "| 4.0000 | A12            |       0.0509 |   9.1931 |            0.0009 |           -0.0352 |    0.0159 |    0.0524 |    0.0029 |    0.0117 |\n",
      "| 3.0000 | A9             |       0.0528 |   9.1832 |            0.0349 |            0.0033 |    0.0676 |    0.0351 |    0.0171 |    0.0117 |\n",
      "| 2.0000 | A6             |       0.2082 |   0.1485 |           -0.0599 |            0.0031 |    0.0781 |    0.0494 |    0.0136 |    0.0100 |\n",
      "| 1.0000 | A2             |       0.2258 | 660.9390 |            0.1458 |           -0.0177 |    0.1801 |    0.1020 |    0.0580 |    0.0324 |\n",
      "| 0.0000 | A3             |       0.2330 | 665.2065 |            0.1563 |           -0.0079 |    0.1912 |    0.0952 |    0.0677 |    0.0336 |\n",
      "+--------+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "*******\n",
      "Legend:\n",
      "Importance = Feature Importance\n",
      "     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\n",
      "FR_delta = Feature Response Delta Amount\n",
      "     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\n",
      "FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\n",
      "FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\n",
      "FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\n",
      "FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\n",
      "FRD_abs = Feature Response From Decrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "FRI_abs = Feature Response From Incrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "*******\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<auto_ml.predictor.Predictor at 0x29ad7c566d8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://auto-ml.readthedocs.io/en/latest/\n",
    "from auto_ml import Predictor\n",
    "\n",
    "# If you pass in any categorical data as a number, tell us here and we'll take care of it.\n",
    "col_desc_dictionary = {\"Class\": \"output\"}\n",
    "\n",
    "# Can pass in type_of_estimator='regressor' as well\n",
    "ml_predictor = Predictor(type_of_estimator='classifier', column_descriptions=col_desc_dictionary)\n",
    "\n",
    "# Wait for the machine to learn all the complex and beautiful patterns in your data...\n",
    "ml_predictor.train(training, ml_for_analytics=True)\n",
    "\n",
    "# And this time, in your shell, it will print out the results for what it found was useful in making predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 0.9999998807907104, False], [5, 0.9999998807907104, False], [20, 0.9999998807907104, False], [5, 0.9999998807907104, False], [20, 0.9999998807907104, False]]\n",
      "Validation 0 of 25092 - 0.000000%\n"
     ]
    }
   ],
   "source": [
    "# Where new_data is a single dictionary, or a DataFrame\n",
    "predictions = ml_predictor.predict(testing)\n",
    "classes = [[i,k] for i,k in enumerate(testing['Class'])]\n",
    "validation = list(map(lambda p: [p[1],predictions[p[0]],p[1] == predictions[p[0]]], classes))\n",
    "print(validation[0:5])\n",
    "total = len(validation)\n",
    "ok = len(list(filter(lambda k: k[2], validation)))\n",
    "perc = 100 / total * ok\n",
    "print(\"Validation %d of %d - %f%%\" % (ok, total, perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT\n",
    "https://rhiever.github.io/tpot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tpot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-eb954fd09774>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtpot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTPOTClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtesting\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tpot'"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = training[training.columns[:-1]]\n",
    "X_test = testing[testing.columns[:-1]]\n",
    "y_train = training[training.columns[-1]]\n",
    "y_test = testing[testing.columns[-1]]\n",
    "\n",
    "pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5,\n",
    "                                    random_state=42, verbosity=2)\n",
    "pipeline_optimizer.fit(X_train, y_train)\n",
    "print(pipeline_optimizer.score(X_test, y_test))\n",
    "pipeline_optimizer.export('tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A1  A2  A3  A4  A5   A6  A7  A8  A9  A10  A11  A12  Class  predict\n",
      "0   1   1   2  40  10  101   1   5   2    1    6    2      5        5\n",
      "1   1   1   2  43  10    7  10   1  44   10    2   44      5        5\n",
      "2   1   1  43  40  10   35  11   4  45   11    4   45     20        5\n",
      "3   1   1  43  41  10   13  10   7  42   10    7   42      5        5\n",
      "4   1   1  43  43  10   44  12   6  49   12    6   49     20        5\n",
      "Ok 22237 / 25092 88.621872%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = training.drop('Class', axis=1).values\n",
    "training_target = training['Class'].values\n",
    "\n",
    "# Score on the training set was:0.8861336642426755\n",
    "exported_pipeline = ExtraTreesClassifier(bootstrap=False, \n",
    "                                            criterion=\"entropy\", max_features=0.8500000000000001, min_samples_leaf=20, min_samples_split=15, n_estimators=100)\n",
    "\n",
    "exported_pipeline.fit(features, training_target)\n",
    "\n",
    "t = testing.drop('Class', axis=1)\n",
    "try:\n",
    "    testing.drop('predict', axis=1)\n",
    "except:\n",
    "    pass\n",
    "t = t.values\n",
    "testing['predict'] = exported_pipeline.predict(t)\n",
    "testing.to_csv('prediction.csv')\n",
    "print(testing.head())\n",
    "ok= len(testing[testing.Class == testing.predict].values)\n",
    "total = len(t)\n",
    "accuracy = 100 / total * ok\n",
    "print(\"Ok %d / %d %f%%\" % (ok, total, accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test predictor service\n",
    "run `python PredictorService.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Ok!\"\\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "requests.get('http://localhost:5002/test').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda custom (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
